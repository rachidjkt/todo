{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fa481569",
      "metadata": {
        "id": "fa481569"
      },
      "source": [
        "# SlowFast\n",
        "\n",
        "*Author: FAIR PyTorchVideo*\n",
        "\n",
        "**SlowFast networks pretrained on the Kinetics 400 dataset**\n",
        "\n",
        "\n",
        "### Example Usage\n",
        "\n",
        "#### Imports\n",
        "\n",
        "Load the model:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore\n",
        "!pip install av"
      ],
      "metadata": {
        "id": "n_xCRg609Hnv",
        "outputId": "a0b6b6f8-04f1-4562-9fd5-d56e8b9a446c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "n_xCRg609Hnv",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.7->fvcore) (4.14.1)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=cdc7c12e41cb7b29af1da3ada1b6b5b7cb2aca06314a8f84c27ed61d564cab63\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=4bf0e4b6b7b3b7b2a52bcddf3c9bf588899ff98c0222352120c4fa05e51a6abb\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.2.0 yacs-0.1.8\n",
            "Collecting av\n",
            "  Downloading av-15.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Downloading av-15.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "99e832d6",
      "metadata": {
        "id": "99e832d6",
        "outputId": "81ddd4c9-653f-4c2d-c01c-dad212d43a34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/pytorchvideo/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/pytorchvideo/model_zoo/kinetics/SLOWFAST_8x8_R50.pyth\" to /root/.cache/torch/hub/checkpoints/SLOWFAST_8x8_R50.pyth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 264M/264M [00:01<00:00, 221MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Choose the `slowfast_r50` model\n",
        "model = torch.hub.load('facebookresearch/pytorchvideo', 'slowfast_r50', pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69054695",
      "metadata": {
        "id": "69054695"
      },
      "source": [
        "Import remaining functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f5661afb",
      "metadata": {
        "id": "f5661afb",
        "outputId": "857eb466-4af7-4ee0-b3b4-0f1a9b99ae82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
            "  warnings.warn(\n",
            "/root/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/data/frame_video.py:106: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  return [int(c) if c.isdigit() else c for c in re.split(\"(\\d+)\", text)]\n"
          ]
        }
      ],
      "source": [
        "from typing import Dict\n",
        "import json\n",
        "import urllib\n",
        "from torchvision.transforms import Compose, Lambda\n",
        "from torchvision.transforms._transforms_video import (\n",
        "    CenterCropVideo,\n",
        "    NormalizeVideo,\n",
        ")\n",
        "from pytorchvideo.data.encoded_video import EncodedVideo\n",
        "from pytorchvideo.transforms import (\n",
        "    ApplyTransformToKey,\n",
        "    ShortSideScale,\n",
        "    UniformTemporalSubsample,\n",
        "    UniformCropVideo\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31d290f2",
      "metadata": {
        "id": "31d290f2"
      },
      "source": [
        "#### Setup\n",
        "\n",
        "Set the model to eval mode and move to desired device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "344f86e1",
      "metadata": {
        "attributes": {
          "classes": [
            "python "
          ],
          "id": ""
        },
        "id": "344f86e1"
      },
      "outputs": [],
      "source": [
        "# Set to GPU or CPU\n",
        "device = \"cpu\"\n",
        "model = model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1afe37e",
      "metadata": {
        "id": "b1afe37e"
      },
      "source": [
        "Download the id to label mapping for the Kinetics 400 dataset on which the torch hub models were trained. This will be used to get the category label names from the predicted class ids."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "474dcf8d",
      "metadata": {
        "id": "474dcf8d"
      },
      "outputs": [],
      "source": [
        "json_url = \"https://dl.fbaipublicfiles.com/pyslowfast/dataset/class_names/kinetics_classnames.json\"\n",
        "json_filename = \"kinetics_classnames.json\"\n",
        "try: urllib.URLopener().retrieve(json_url, json_filename)\n",
        "except: urllib.request.urlretrieve(json_url, json_filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3fbdadaf",
      "metadata": {
        "id": "3fbdadaf"
      },
      "outputs": [],
      "source": [
        "with open(json_filename, \"r\") as f:\n",
        "    kinetics_classnames = json.load(f)\n",
        "\n",
        "# Create an id to label name mapping\n",
        "kinetics_id_to_classname = {}\n",
        "for k, v in kinetics_classnames.items():\n",
        "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c04836fb",
      "metadata": {
        "id": "c04836fb"
      },
      "source": [
        "#### Define input transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c242c0c0",
      "metadata": {
        "id": "c242c0c0"
      },
      "outputs": [],
      "source": [
        "side_size = 256\n",
        "mean = [0.45, 0.45, 0.45]\n",
        "std = [0.225, 0.225, 0.225]\n",
        "crop_size = 256\n",
        "num_frames = 32\n",
        "sampling_rate = 2\n",
        "frames_per_second = 30\n",
        "slowfast_alpha = 4\n",
        "num_clips = 10\n",
        "num_crops = 3\n",
        "\n",
        "class PackPathway(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Transform for converting video frames as a list of tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, frames: torch.Tensor):\n",
        "        fast_pathway = frames\n",
        "        # Perform temporal sampling from the fast pathway.\n",
        "        slow_pathway = torch.index_select(\n",
        "            frames,\n",
        "            1,\n",
        "            torch.linspace(\n",
        "                0, frames.shape[1] - 1, frames.shape[1] // slowfast_alpha\n",
        "            ).long(),\n",
        "        )\n",
        "        frame_list = [slow_pathway, fast_pathway]\n",
        "        return frame_list\n",
        "\n",
        "transform =  ApplyTransformToKey(\n",
        "    key=\"video\",\n",
        "    transform=Compose(\n",
        "        [\n",
        "            UniformTemporalSubsample(num_frames),\n",
        "            Lambda(lambda x: x/255.0),\n",
        "            NormalizeVideo(mean, std),\n",
        "            ShortSideScale(\n",
        "                size=side_size\n",
        "            ),\n",
        "            CenterCropVideo(crop_size),\n",
        "            PackPathway()\n",
        "        ]\n",
        "    ),\n",
        ")\n",
        "\n",
        "# The duration of the input clip is also specific to the model.\n",
        "clip_duration = (num_frames * sampling_rate)/frames_per_second"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7318ff38",
      "metadata": {
        "id": "7318ff38"
      },
      "source": [
        "#### Run Inference\n",
        "\n",
        "Download an example video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "848e5a56",
      "metadata": {
        "id": "848e5a56"
      },
      "outputs": [],
      "source": [
        "#regular mp4\n",
        "url_link = \"https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4\"\n",
        "video_path = 'archery.mp4'\n",
        "try: urllib.URLopener().retrieve(url_link, video_path)\n",
        "except: urllib.request.urlretrieve(url_link, video_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for google drive video\n",
        "import urllib.request\n",
        "\n",
        "# Google Drive direct download link\n",
        "url_link = \"https://drive.google.com/uc?export=download&id=1S5kDR2XdvIMSyMalGfXHbC_CI6Hx3PLG\"\n",
        "video_path = \"archery.mp4\"\n",
        "urllib.request.urlretrieve(url_link, video_path)\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "video = EncodedVideo.from_path(video_path)\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "video_data = transform(video_data)\n",
        "inputs = video_data[\"video\"]\n",
        "inputs = [i.to(device)[None, ...] for i in inputs]"
      ],
      "metadata": {
        "id": "llq6ube_94lz"
      },
      "id": "llq6ube_94lz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "326b7a94",
      "metadata": {
        "id": "326b7a94"
      },
      "source": [
        "Load the video and transform it to the input format required by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b09c420b",
      "metadata": {
        "id": "b09c420b"
      },
      "outputs": [],
      "source": [
        "# Select the duration of the clip to load by specifying the start and end duration\n",
        "# The start_sec should correspond to where the action occurs in the video\n",
        "start_sec = 0\n",
        "end_sec = start_sec + clip_duration\n",
        "\n",
        "# Initialize an EncodedVideo helper class and load the video\n",
        "video = EncodedVideo.from_path(video_path)\n",
        "\n",
        "# Load the desired clip\n",
        "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
        "\n",
        "# Apply a transform to normalize the video input\n",
        "video_data = transform(video_data)\n",
        "\n",
        "# Move the inputs to the desired device\n",
        "inputs = video_data[\"video\"]\n",
        "inputs = [i.to(device)[None, ...] for i in inputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef00f1e7",
      "metadata": {
        "id": "ef00f1e7"
      },
      "source": [
        "#### Get Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a36d8fe2",
      "metadata": {
        "id": "a36d8fe2",
        "outputId": "45af6686-53dd-4e02-8ee6-4249010b3663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 predicted labels: archery, throwing axe, playing paintball, disc golfing, riding or walking with horse\n"
          ]
        }
      ],
      "source": [
        "# Pass the input clip through the model\n",
        "preds = model(inputs)\n",
        "\n",
        "# Get the predicted classes\n",
        "post_act = torch.nn.Softmax(dim=1)\n",
        "preds = post_act(preds)\n",
        "pred_classes = preds.topk(k=5).indices[0]\n",
        "\n",
        "# Map the predicted classes to the label names\n",
        "pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes]\n",
        "print(\"Top 5 predicted labels: %s\" % \", \".join(pred_class_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf44a366",
      "metadata": {
        "id": "cf44a366"
      },
      "source": [
        "### Model Description\n",
        "SlowFast model architectures are based on [1] with pretrained weights using the 8x8 setting\n",
        "on the Kinetics dataset.\n",
        "\n",
        "| arch | depth | frame length x sample rate | top 1 | top 5 | Flops (G) | Params (M) |\n",
        "| --------------- | ----------- | ----------- | ----------- | ----------- | ----------- |  ----------- | ----------- |\n",
        "| SlowFast | R50   | 8x8                        | 76.94 | 92.69 | 65.71     | 34.57      |\n",
        "| SlowFast | R101  | 8x8                        | 77.90 | 93.27 | 127.20    | 62.83      |\n",
        "\n",
        "\n",
        "### References\n",
        "[1] Christoph Feichtenhofer et al, \"SlowFast Networks for Video Recognition\"\n",
        "https://arxiv.org/pdf/1812.03982.pdf"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}